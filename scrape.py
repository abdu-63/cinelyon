#!/usr/bin/env python3
"""
Script de scraping autonome pour r√©cup√©rer les s√©ances de cin√©ma.
Sauvegarde les donn√©es dans movies.json.
Con√ßu pour √™tre ex√©cut√© via GitHub Actions.
Supporte le scraping incr√©mental et la reprise apr√®s √©chec.
"""

import argparse
import json
import logging
import os
import time
from datetime import datetime, timedelta

from dotenv import load_dotenv

from modules.Classes import Theater, TMDB_CACHE_FILE

load_dotenv(".env")

THEATERS_JSON = os.environ.get("THEATERS", "[]")

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
logger = logging.getLogger(__name__)

TMDB_API_KEY = os.environ.get("TMDB_API_KEY", "")
OUTPUT_FILE = "movies.json"
DAYS_TO_SCRAPE = 10
DELAY_BETWEEN_THEATERS = 2  # D√©lai en secondes entre chaque cin√©ma


def get_showtimes(theaters: list[Theater], date: datetime) -> list[dict]:
    """R√©cup√®re les s√©ances pour une date donn√©e (s√©quentiel avec d√©lai)."""
    showtimes_list = []

    for i, theater in enumerate(theaters):
        try:
            showtimes_list.extend(theater.getShowtimes(date))
        except Exception as e:
            logger.error(f"Erreur pour {theater.name}: {e}")

        # D√©lai entre les requ√™tes pour √©viter le rate limiting (sauf pour le dernier)
        if i < len(theaters) - 1:
            time.sleep(DELAY_BETWEEN_THEATERS)

    data = {}

    for showtime in showtimes_list:
        movie = showtime.movie
        theater = showtime.theater

        if movie.title not in data.keys():
            data[movie.title] = {
                "title": movie.title,
                "release_year": movie.release_year,
                "duree": movie.runtime,
                "rating": movie.rating,
                "genres": ", ".join(movie.genres),
                "realisateur": movie.director,
                "synopsis": movie.synopsis,
                "affiche": movie.affiche,
                "director": movie.director,
                "wantToSee": movie.wantToSee,
                "url": movie.letterboxd_url,
                "seances": {},
            }

        if theater.name not in data[movie.title]["seances"].keys():
            data[movie.title]["seances"][theater.name] = []

        data[movie.title]["seances"][theater.name].append(
            {
                "time": showtime.startsAt.strftime("%H:%M"),
                "lang": showtime.language,
                "format": showtime.format,
                "ticketing_url": showtime.ticketing_url,
            }
        )

    movies = list(data.values())
    movies = sorted(movies, key=lambda x: x["wantToSee"], reverse=True)

    return movies


def load_existing_data() -> dict:
    """Charge les donn√©es existantes si disponibles."""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {"generated_at": None, "days": []}


def save_data(data: dict):
    """Sauvegarde les donn√©es dans movies.json."""
    data["generated_at"] = datetime.now().isoformat()
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_dates_to_scrape(existing_data: dict) -> list[str]:
    """D√©termine les dates √† scraper (manquantes ou √† mettre √† jour)."""
    today = datetime.today().date()
    target_dates = set()

    for i in range(DAYS_TO_SCRAPE):
        date = today + timedelta(days=i)
        target_dates.add(date.strftime("%Y-%m-%d"))

    existing_dates = set()
    for day in existing_data.get("days", []):
        date_str = day.get("date", "")
        # Garder les dates existantes seulement si elles sont encore dans la p√©riode cible
        if date_str in target_dates:
            existing_dates.add(date_str)

    # Retourner les dates manquantes, tri√©es
    missing_dates = target_dates - existing_dates
    return sorted(list(missing_dates))


def clean_old_dates(data: dict) -> dict:
    """Supprime les dates pass√©es et hors de la p√©riode de scraping."""
    today = datetime.today().date()
    valid_dates = set()

    for i in range(DAYS_TO_SCRAPE):
        date = today + timedelta(days=i)
        valid_dates.add(date.strftime("%Y-%m-%d"))

    data["days"] = [day for day in data.get("days", []) if day.get("date") in valid_dates]
    return data


def check_missing_data(existing_data: dict) -> tuple[set[str], set[str]]:
    """V√©rifie si des films ont des donn√©es manquantes (affiche, synopsis).
    Retourne les dates √† rescraper et les titres de films √† supprimer du cache TMDB."""
    dates_with_missing_data = set()
    films_to_clear_from_cache = set()
    
    for day in existing_data.get("days", []):
        date_str = day.get("date", "")
        movies = day.get("movies", [])
        
        for movie in movies:
            title = movie.get("title", "Inconnu")
            year = movie.get("release_year", "")
            
            affiche = movie.get("affiche", "")
            has_missing_data = False
            
            if not affiche or affiche == "/static/images/nocontent.png":
                logger.info(f"   üì∑ Affiche manquante pour '{title}' ({date_str})")
                has_missing_data = True
            
            synopsis = movie.get("synopsis", "")
            if not synopsis or synopsis == "Synopsis non disponible":
                logger.info(f"   üìù Synopsis manquant pour '{title}' ({date_str})")
                has_missing_data = True
            
            if has_missing_data:
                dates_with_missing_data.add(date_str)
                films_to_clear_from_cache.add(f"{title}|{year}")
    
    return dates_with_missing_data, films_to_clear_from_cache


def main():
    # Parser d'arguments
    parser = argparse.ArgumentParser(description="Script de scraping des s√©ances de cin√©ma")
    parser.add_argument("--force", action="store_true", help="Forcer le rescraping complet de toutes les dates")
    parser.add_argument("--clear-cache", action="store_true", help="Vider le cache TMDB avant le scraping")
    args = parser.parse_args()

    logger.info("üé¨ D√©marrage du scraping des s√©ances de cin√©ma...")

    # Vider le cache TMDB si demand√©
    if args.clear_cache:
        if os.path.exists(TMDB_CACHE_FILE):
            os.remove(TMDB_CACHE_FILE)
            logger.info("üóëÔ∏è Cache TMDB supprim√©")

    theaters_config = json.loads(THEATERS_JSON)

    if not theaters_config:
        logger.error("‚ùå Aucun cin√©ma configur√©. V√©rifiez la variable THEATERS.")
        return

    if not TMDB_API_KEY:
        logger.warning("‚ö†Ô∏è TMDB_API_KEY non configur√©e ! Les donn√©es TMDB seront manquantes.")

    theaters = []
    for theater_data in theaters_config:
        theaters.append(
            Theater(
                {
                    "name": theater_data["name"],
                    "internalId": theater_data["id"],
                    "latitude": theater_data["latitude"],
                    "longitude": theater_data["longitude"],
                    "location": None,
                }
            )
        )

    logger.info(f"üìç {len(theaters)} cin√©ma(s) configur√©(s)")

    # Charger les donn√©es existantes (sauf si --force)
    if args.force:
        existing_data = {"generated_at": None, "days": []}
        logger.info("üîÑ Mode force activ√© - rescraping complet")
    else:
        existing_data = load_existing_data()
        existing_data = clean_old_dates(existing_data)

    # D√©terminer les dates √† scraper
    dates_to_scrape = set(get_dates_to_scrape(existing_data))
    
    # V√©rifier les donn√©es manquantes
    if not args.force:
        logger.info("üîç V√©rification des donn√©es manquantes...")
        dates_with_missing, films_to_clear = check_missing_data(existing_data)
        if dates_with_missing:
            logger.info(f"   ‚ö†Ô∏è {len(dates_with_missing)} date(s) avec donn√©es manquantes")
            logger.info(f"   üóëÔ∏è {len(films_to_clear)} film(s) √† supprimer du cache TMDB")
            
            titles_to_clear = {key.split("|")[0] for key in films_to_clear}
            
            if os.path.exists(TMDB_CACHE_FILE):
                try:
                    with open(TMDB_CACHE_FILE, "r", encoding="utf-8") as f:
                        tmdb_cache = json.load(f)
                    
                    keys_to_delete = []
                    for cache_key in tmdb_cache.keys():
                        cache_title = cache_key.split("|")[0]
                        if cache_title in titles_to_clear:
                            keys_to_delete.append(cache_key)
                    
                    for key in keys_to_delete:
                        del tmdb_cache[key]
                        logger.info(f"      üßπ Cache supprim√© pour: {key.split('|')[0]}")
                    
                    with open(TMDB_CACHE_FILE, "w", encoding="utf-8") as f:
                        json.dump(tmdb_cache, f, ensure_ascii=False, indent=2)
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Erreur nettoyage cache: {e}")
            
            dates_to_scrape.update(dates_with_missing)
            existing_data["days"] = [day for day in existing_data.get("days", []) 
                                      if day.get("date") not in dates_with_missing]
    
    dates_to_scrape = sorted(list(dates_to_scrape))

    if not dates_to_scrape:
        logger.info("‚úÖ Toutes les donn√©es sont √† jour, aucun scraping n√©cessaire.")
        logger.info("   Utilisez --force pour forcer le rescraping")
        save_data(existing_data)
        return

    logger.info(f"üìÖ {len(dates_to_scrape)} jour(s) √† scraper (donn√©es existantes conserv√©es)")

    # Cr√©er un dictionnaire des jours existants pour acc√®s rapide
    existing_days = {day["date"]: day for day in existing_data.get("days", [])}

    for date_str in dates_to_scrape:
        date = datetime.strptime(date_str, "%Y-%m-%d")

        logger.info(f"üìÖ R√©cup√©ration des s√©ances pour {date_str}...")

        try:
            movies = get_showtimes(theaters, date)

            existing_days[date_str] = {"date": date_str, "movies": movies}

            logger.info(f"   ‚úÖ {len(movies)} film(s) r√©cup√©r√©(s)")

            # Sauvegarder apr√®s chaque jour pour pouvoir reprendre en cas d'√©chec
            existing_data["days"] = sorted(existing_days.values(), key=lambda x: x["date"])
            save_data(existing_data)

            # Petit d√©lai pour √©viter le rate limiting
            time.sleep(1)

        except Exception as e:
            logger.error(f"‚ùå Erreur pour {date_str}: {e}")
            logger.warning("üíæ Progr√®s sauvegard√©. Relancez le script pour continuer.")
            # Sauvegarder le progr√®s avant de quitter
            existing_data["days"] = sorted(existing_days.values(), key=lambda x: x["date"])
            save_data(existing_data)
            raise

    logger.info(f"‚úÖ Scraping termin√© et sauvegard√© dans {OUTPUT_FILE}")
    total_movies = sum(len(day["movies"]) for day in existing_data["days"])
    logger.info(f"üìä Total: {total_movies} entr√©es de films sur {len(existing_data['days'])} jours")


if __name__ == "__main__":
    main()
