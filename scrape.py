#!/usr/bin/env python3
"""
Script de scraping autonome pour rÃ©cupÃ©rer les sÃ©ances de cinÃ©ma.
Sauvegarde les donnÃ©es dans movies.json.
ConÃ§u pour Ãªtre exÃ©cutÃ© via GitHub Actions.
Supporte le scraping incrÃ©mental et la reprise aprÃ¨s Ã©chec.
"""

import logging
import json
import os
import time
from datetime import datetime, timedelta

from dotenv import load_dotenv

from modules.Classes import Theater

load_dotenv(".env")

THEATERS_JSON = os.environ.get("THEATERS", "[]")

# Configuration du logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
logger = logging.getLogger(__name__)

TMDB_API_KEY = os.environ.get("TMDB_API_KEY", "")
OUTPUT_FILE = "movies.json"
DAYS_TO_SCRAPE = 7
DELAY_BETWEEN_THEATERS = 2  # DÃ©lai en secondes entre chaque cinÃ©ma


def get_showtimes(theaters: list[Theater], date: datetime) -> list[dict]:
    """RÃ©cupÃ¨re les sÃ©ances pour une date donnÃ©e (sÃ©quentiel avec dÃ©lai)."""
    showtimes_list = []

    for i, theater in enumerate(theaters):
        try:
            showtimes_list.extend(theater.getShowtimes(date))
        except Exception as e:
            logger.error(f"Erreur pour {theater.name}: {e}")

        # DÃ©lai entre les requÃªtes pour Ã©viter le rate limiting (sauf pour le dernier)
        if i < len(theaters) - 1:
            time.sleep(DELAY_BETWEEN_THEATERS)

    data = {}

    for showtime in showtimes_list:
        movie = showtime.movie
        theater = showtime.theater

        if movie.title not in data.keys():
            data[movie.title] = {
                "title": movie.title,
                "release_year": movie.release_year,
                "duree": movie.runtime,
                "rating": movie.rating,
                "genres": ", ".join(movie.genres),
                "realisateur": movie.director,
                "synopsis": movie.synopsis,
                "affiche": movie.affiche,
                "director": movie.director,
                "wantToSee": movie.wantToSee,
                "url": movie.letterboxd_url,
                "seances": {},
            }

        if theater.name not in data[movie.title]["seances"].keys():
            data[movie.title]["seances"][theater.name] = []

        data[movie.title]["seances"][theater.name].append(
            {
                "time": showtime.startsAt.strftime("%H:%M"),
                "lang": showtime.language,
                "format": showtime.format,
                "ticketing_url": showtime.ticketing_url,
            }
        )

    movies = list(data.values())
    movies = sorted(movies, key=lambda x: x["wantToSee"], reverse=True)

    return movies


def load_existing_data() -> dict:
    """Charge les donnÃ©es existantes si disponibles."""
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {"generated_at": None, "days": []}


def save_data(data: dict):
    """Sauvegarde les donnÃ©es dans movies.json."""
    data["generated_at"] = datetime.now().isoformat()
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_dates_to_scrape(existing_data: dict) -> list[str]:
    """DÃ©termine les dates Ã  scraper (manquantes ou Ã  mettre Ã  jour)."""
    today = datetime.today().date()
    target_dates = set()

    for i in range(DAYS_TO_SCRAPE):
        date = today + timedelta(days=i)
        target_dates.add(date.strftime("%Y-%m-%d"))

    existing_dates = set()
    for day in existing_data.get("days", []):
        date_str = day.get("date", "")
        # Garder les dates existantes seulement si elles sont encore dans la pÃ©riode cible
        if date_str in target_dates:
            existing_dates.add(date_str)

    # Retourner les dates manquantes, triÃ©es
    missing_dates = target_dates - existing_dates
    return sorted(list(missing_dates))


def clean_old_dates(data: dict) -> dict:
    """Supprime les dates passÃ©es et hors de la pÃ©riode de scraping."""
    today = datetime.today().date()
    valid_dates = set()

    for i in range(DAYS_TO_SCRAPE):
        date = today + timedelta(days=i)
        valid_dates.add(date.strftime("%Y-%m-%d"))

    data["days"] = [day for day in data.get("days", []) if day.get("date") in valid_dates]
    return data


def main():
    logger.info("ğŸ¬ DÃ©marrage du scraping des sÃ©ances de cinÃ©ma...")

    theaters_config = json.loads(THEATERS_JSON)

    if not theaters_config:
        logger.error("âŒ Aucun cinÃ©ma configurÃ©. VÃ©rifiez la variable THEATERS.")
        return

    if not TMDB_API_KEY:
        logger.warning("âš ï¸ TMDB_API_KEY non configurÃ©e ! Les donnÃ©es TMDB seront manquantes.")

    theaters = []
    for theater_data in theaters_config:
        theaters.append(
            Theater(
                {
                    "name": theater_data["name"],
                    "internalId": theater_data["id"],
                    "latitude": theater_data["latitude"],
                    "longitude": theater_data["longitude"],
                    "location": None,
                }
            )
        )

    logger.info(f"ğŸ“ {len(theaters)} cinÃ©ma(s) configurÃ©(s)")

    # Charger les donnÃ©es existantes
    existing_data = load_existing_data()

    # Nettoyer les dates obsolÃ¨tes
    existing_data = clean_old_dates(existing_data)

    # DÃ©terminer les dates Ã  scraper
    dates_to_scrape = get_dates_to_scrape(existing_data)

    if not dates_to_scrape:
        logger.info("âœ… Toutes les donnÃ©es sont Ã  jour, aucun scraping nÃ©cessaire.")
        save_data(existing_data)
        return

    logger.info(f"ğŸ“… {len(dates_to_scrape)} jour(s) Ã  scraper (donnÃ©es existantes conservÃ©es)")

    # CrÃ©er un dictionnaire des jours existants pour accÃ¨s rapide
    existing_days = {day["date"]: day for day in existing_data.get("days", [])}

    for date_str in dates_to_scrape:
        date = datetime.strptime(date_str, "%Y-%m-%d")

        logger.info(f"ğŸ“… RÃ©cupÃ©ration des sÃ©ances pour {date_str}...")

        try:
            movies = get_showtimes(theaters, date)

            existing_days[date_str] = {"date": date_str, "movies": movies}

            logger.info(f"   âœ… {len(movies)} film(s) rÃ©cupÃ©rÃ©(s)")

            # Sauvegarder aprÃ¨s chaque jour pour pouvoir reprendre en cas d'Ã©chec
            existing_data["days"] = sorted(existing_days.values(), key=lambda x: x["date"])
            save_data(existing_data)

            # Petit dÃ©lai pour Ã©viter le rate limiting
            time.sleep(1)

        except Exception as e:
            logger.error(f"âŒ Erreur pour {date_str}: {e}")
            logger.warning("ğŸ’¾ ProgrÃ¨s sauvegardÃ©. Relancez le script pour continuer.")
            # Sauvegarder le progrÃ¨s avant de quitter
            existing_data["days"] = sorted(existing_days.values(), key=lambda x: x["date"])
            save_data(existing_data)
            raise

    logger.info(f"âœ… Scraping terminÃ© et sauvegardÃ© dans {OUTPUT_FILE}")
    total_movies = sum(len(day["movies"]) for day in existing_data["days"])
    logger.info(f"ğŸ“Š Total: {total_movies} entrÃ©es de films sur {len(existing_data['days'])} jours")


if __name__ == "__main__":
    main()
